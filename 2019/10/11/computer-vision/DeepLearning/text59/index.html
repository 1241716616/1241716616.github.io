<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="一只行走的红烧肉" href="http://example.com/rss.xml"><link rel="alternate" type="application/atom+xml" title="一只行走的红烧肉" href="http://example.com/atom.xml"><link rel="alternate" type="application/json" title="一只行走的红烧肉" href="http://example.com/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><meta name="keywords" content="PyTorch 深度学习"><link rel="canonical" href="http://example.com/2019/10/11/computer-vision/DeepLearning/text59/"><title>PyTorch 快速入门 - 深度学习 - 计算机视觉 | 业余天王 = 一只行走的红烧肉 = 流水账</title><meta name="generator" content="Hexo 5.2.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">PyTorch 快速入门</h1><div class="meta"><span class="item" title="创建时间：2019-10-11 00:00:00"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2019-10-11T00:00:00+08:00">2019-10-11</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>9.2k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>8 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">业余天王</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="http://pic.yupoo.com/1241716616/c3a0aa9f/bcbb6c2c.jpg"></li><li class="item" data-background-image="http://pic.yupoo.com/1241716616/4bd7d1c3/f559f80e.jpg"></li><li class="item" data-background-image="http://pic.yupoo.com/1241716616/b5eb9440/159f615f.jpg"></li><li class="item" data-background-image="http://pic.yupoo.com/1241716616/caf26d93/80e1f66e.jpg"></li><li class="item" data-background-image="http://pic.yupoo.com/1241716616/25709aeb/18a8af42.jpg"></li><li class="item" data-background-image="http://pic.yupoo.com/1241716616/5dce46ac/2b8e07e9.jpg"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/computer-vision/" itemprop="item" rel="index" title="分类于 计算机视觉"><span itemprop="name">计算机视觉</span></a><meta itemprop="position" content="1"></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/computer-vision/DeepLearning/" itemprop="item" rel="index" title="分类于 深度学习"><span itemprop="name">深度学习</span></a><meta itemprop="position" content="2"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="http://example.com/2019/10/11/computer-vision/DeepLearning/text59/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/author.jpg"><meta itemprop="name" content="Mr.Wang"><meta itemprop="description" content="流水账, 一只行走的红烧肉"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="一只行走的红烧肉"></span><div class="body md" itemprop="articleBody"><h1 id="pytorch-快速入门"><a class="anchor" href="#pytorch-快速入门">#</a> PyTorch 快速入门</h1><h2 id="数据操作"><a class="anchor" href="#数据操作">#</a> 数据操作</h2><p>Tensor 是 PyTorch 中重要的数据结构，可认为是一个高维数组。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）以及更高维的数组。Tensor 和 Numpy 的 ndarrays 类似，但 Tensor 可以使用 GPU 进行加速。Tensor 的使用和 Numpy 及 Matlab 的接口十分相似，下面通过几个例子来看看 Tensor 的基本使用。</p><h3 id="创建-tensor"><a class="anchor" href="#创建-tensor">#</a> 创建 Tensor</h3><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line">x=t.Tensor(<span class="number">5</span>,<span class="number">3</span>)  <span class="comment">#构建 5x3 矩阵，只是分配了空间，为初始化</span></span><br><span class="line">x=t.Tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">print(x)</span><br><span class="line">x=t.rand(<span class="number">5</span>,<span class="number">3</span>)  <span class="comment"># 使用 [0,1] 均匀分布随机初始化二维数组</span></span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">x=t.zeros(<span class="number">5</span>,<span class="number">3</span>,dtype=t.long)  <span class="comment">#创建一个5x3的long型全0的Tensor</span></span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><br>输出结果：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">1.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">4.</span>]])</span><br><span class="line">tensor([[<span class="number">0.2980</span>, <span class="number">0.6733</span>, <span class="number">0.3038</span>],</span><br><span class="line">        [<span class="number">0.0063</span>, <span class="number">0.9655</span>, <span class="number">0.8104</span>],</span><br><span class="line">        [<span class="number">0.3723</span>, <span class="number">0.7130</span>, <span class="number">0.7597</span>],</span><br><span class="line">        [<span class="number">0.7961</span>, <span class="number">0.3731</span>, <span class="number">0.5026</span>],</span><br><span class="line">        [<span class="number">0.1268</span>, <span class="number">0.6352</span>, <span class="number">0.7748</span>]])</span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br></pre></td></tr></table></figure><p></p><p>还可以通过现有的 Tensor 来创建，此方法会默认重用输入 Tensor 的一些属性，例如数据类型，除非自定义数据类型。</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line">x=x.new_ones(<span class="number">5</span>,<span class="number">3</span>,dtype=t.float64)  <span class="comment"># 返回的tensor默认具有相同的torch.dtype和torch.device</span></span><br><span class="line">print(x)</span><br><span class="line">x=t.rand_like(x,dtype=t.<span class="built_in">float</span>)  <span class="comment"># 指定新的数据类型</span></span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><br>输出结果：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]], dtype=torch.float64)</span><br><span class="line">tensor([[<span class="number">0.4652</span>, <span class="number">0.2571</span>, <span class="number">0.7920</span>],</span><br><span class="line">        [<span class="number">0.5246</span>, <span class="number">0.6747</span>, <span class="number">0.5710</span>],</span><br><span class="line">        [<span class="number">0.2033</span>, <span class="number">0.3721</span>, <span class="number">0.7832</span>],</span><br><span class="line">        [<span class="number">0.7113</span>, <span class="number">0.2817</span>, <span class="number">0.5628</span>],</span><br><span class="line">        [<span class="number">0.9458</span>, <span class="number">0.0079</span>, <span class="number">0.2302</span>]])</span><br></pre></td></tr></table></figure><br>我们可以通过 shape 或者 size () 来获取 Tensor 的形状:<p></p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line">print(x.size())</span><br><span class="line">print(x.shape)</span><br></pre></td></tr></table></figure><p></p><p>输出结果：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line">torch.Size([<span class="number">5</span>, <span class="number">3</span>])</span><br><span class="line">torch.Size([<span class="number">5</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure><br><span class="red">注意：</span> 返回的 torch.Size 其实就是一个 tuple, 支持所有 tuple 的操作。如 x.size ()[1]，为查看列的个数。<p></p><p>还有很多函数可以创建 Tensor，去翻翻官方 API 就知道了，下表给了一些常用的作参考。</p><p><img data-src="/images/textimages/text59/1.png" alt="1.png | center | 300x0"></p><p>这些创建方法都可以在创建的时候指定数据类型 dtype 和存放 device (cpu/gpu)。</p><h3 id="算术操作"><a class="anchor" href="#算术操作">#</a> 算术操作</h3><p>在 PyTorch 中，同一种操作可能有很多种形式，下面用加法作为例子。</p><p><strong>三种加法的写法：</strong></p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一种</span></span><br><span class="line">x = t.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">y = t.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x + y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二种</span></span><br><span class="line">print(t.add(x,y))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第三种</span></span><br><span class="line">y.add_(x)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure><br>输出都一样：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">1.0669</span>, <span class="number">1.1912</span>, <span class="number">0.2503</span>],</span><br><span class="line">        [<span class="number">1.7781</span>, <span class="number">0.8024</span>, <span class="number">0.5209</span>],</span><br><span class="line">        [<span class="number">1.0002</span>, <span class="number">0.6326</span>, <span class="number">1.8528</span>],</span><br><span class="line">        [<span class="number">0.8280</span>, <span class="number">0.7861</span>, <span class="number">1.4744</span>],</span><br><span class="line">        [<span class="number">0.6947</span>, <span class="number">1.1675</span>, <span class="number">1.1493</span>]])</span><br></pre></td></tr></table></figure><br>注意：]{.red} PyTorch 操作 inplace 版本都有后缀 _ , 例如 x.copy_(y), x.t_()，函数名后面带下划线_ 的函数会修改 Tensor 本身。例如，x.add_(y) 和 x.t_() 会改变 x，但 x.add (y) 和 x.t () 返回一个新的 Tensor， 而 x 不变。<p></p><p><strong>索引：</strong><br>我们还可以使用类似 NumPy 的索引操作来访问 Tensor 的一部分，需要注意的是：索引出来的结果与原数据共享内存，也即修改一个，另一个会跟着修改。</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y=x[<span class="number">0</span>,:]</span><br><span class="line">y+=<span class="number">1</span></span><br><span class="line">print(y)</span><br><span class="line">print(x[<span class="number">0</span>,:])  <span class="comment"># 源tensor也被改了</span></span><br></pre></td></tr></table></figure><p></p><p>输出结果：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">3.7303</span>, <span class="number">3.7781</span>, <span class="number">3.0865</span>])</span><br><span class="line">tensor([<span class="number">3.7303</span>, <span class="number">3.7781</span>, <span class="number">3.0865</span>])</span><br></pre></td></tr></table></figure><br>除了常用的索引选择数据之外，PyTorch 还提供了一些高级的选择函数：下面列举一些常用的，具体可查看官方文档<p></p><p><img data-src="/images/textimages/text59/2.png" alt="2.png | center | 300x0"></p><p><strong>改变形状：</strong><br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y=x.view(<span class="number">15</span>)</span><br><span class="line">z=x.view(-<span class="number">1</span>,<span class="number">5</span>)  <span class="comment"># -1所指的维度可以根据其他维度的值推出来</span></span><br><span class="line">print(x.size(),y.size(),z.size())</span><br></pre></td></tr></table></figure><p></p><p>输出结果<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">5</span>, <span class="number">3</span>]) torch.Size([<span class="number">15</span>]) torch.Size([<span class="number">3</span>, <span class="number">5</span>])</span><br></pre></td></tr></table></figure><br>注意：]{.red} view () 返回的新 Tensor 与源 Tensor 虽然可能有不同的 size，但是是共享 data 的，也即更改其中的一个，另外一个也会跟着改变。(顾名思义，view 仅仅是改变了对这个张量的观察角度，内部数据并未改变)。<p></p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x +=<span class="number">1</span></span><br><span class="line">print(x)</span><br><span class="line">print(y)  <span class="comment">#也加了1</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><br>输出结果<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">12.7303</span>, <span class="number">12.7781</span>, <span class="number">12.0865</span>],</span><br><span class="line">        [<span class="number">10.8539</span>, <span class="number">10.7651</span>, <span class="number">10.4150</span>],</span><br><span class="line">        [<span class="number">10.1946</span>, <span class="number">10.4163</span>, <span class="number">10.8957</span>],</span><br><span class="line">        [<span class="number">10.3376</span>, <span class="number">10.7800</span>, <span class="number">10.9572</span>],</span><br><span class="line">        [<span class="number">10.2153</span>, <span class="number">10.6582</span>, <span class="number">10.8393</span>]])</span><br><span class="line">tensor([<span class="number">12.7303</span>, <span class="number">12.7781</span>, <span class="number">12.0865</span>, <span class="number">10.8539</span>, <span class="number">10.7651</span>, <span class="number">10.4150</span>, <span class="number">10.1946</span>, <span class="number">10.4163</span>, <span class="number">10.8957</span>, <span class="number">10.3376</span>, <span class="number">10.7800</span>, <span class="number">10.9572</span>, <span class="number">10.2153</span>, <span class="number">10.6582</span>, <span class="number">10.8393</span>])</span><br></pre></td></tr></table></figure><p></p><p>所以如果我们想返回一个真正新的副本（即不共享 data 内存）该怎么办呢？Pytorch 还提供了一个 reshape () 可以改变形状，但是此函数并不能保证返回的是其拷贝，所以不推荐使用。推荐先用 clone 创造一个副本然后再使用 view。</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_cp=x.clone().view(<span class="number">15</span>)</span><br><span class="line">x-=<span class="number">1</span></span><br><span class="line">print(x)</span><br><span class="line">print(x_cp)</span><br></pre></td></tr></table></figure><br>输出结果<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">11.7303</span>, <span class="number">11.7781</span>, <span class="number">11.0865</span>],</span><br><span class="line">        [ <span class="number">9.8539</span>,  <span class="number">9.7651</span>,  <span class="number">9.4150</span>],</span><br><span class="line">        [ <span class="number">9.1946</span>,  <span class="number">9.4163</span>,  <span class="number">9.8957</span>],</span><br><span class="line">        [ <span class="number">9.3376</span>,  <span class="number">9.7800</span>,  <span class="number">9.9572</span>],</span><br><span class="line">        [ <span class="number">9.2153</span>,  <span class="number">9.6582</span>,  <span class="number">9.8393</span>]])</span><br><span class="line">tensor([<span class="number">12.7303</span>, <span class="number">12.7781</span>, <span class="number">12.0865</span>, <span class="number">10.8539</span>, <span class="number">10.7651</span>, <span class="number">10.4150</span>, <span class="number">10.1946</span>, <span class="number">10.4163</span>,<span class="number">10.8957</span>, <span class="number">10.3376</span>, <span class="number">10.7800</span>, <span class="number">10.9572</span>, <span class="number">10.2153</span>, <span class="number">10.6582</span>, <span class="number">10.8393</span>])</span><br></pre></td></tr></table></figure><br>使用 clone 还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源 Tensor。<p></p><p>另外一个常用的函数就是 item (), 它可以将一个标量 Tensor 转换成一个 Python number。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x=t.randn(<span class="number">1</span>)</span><br><span class="line">print(x)</span><br><span class="line">print(x.item())</span><br></pre></td></tr></table></figure><br>输出结果<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">0.2765</span>])</span><br><span class="line"><span class="number">0.2765408754348755</span></span><br></pre></td></tr></table></figure><p></p><p><strong>线性代数:</strong><br>另外，PyTorch 还支持一些线性函数，这里提一下，免得用起来的时候自己造轮子，具体用法参考官方文档。如下表所示：</p><p><img data-src="/images/textimages/text59/3.png" alt="3.png | center | 300x0"></p><p>PyTorch 中的 Tensor 支持超过一百种操作，包括转置、索引、切片、数学运算、线性代数、随机数等等，可参考官方文档。</p><h3 id="广播机制"><a class="anchor" href="#广播机制">#</a> 广播机制</h3><p>前面我们看到如何对两个形状相同的 Tensor 做按元素运算。当对两个形状不同的 Tensor 按元素运算时，可能会触发广播（broadcasting）机制：先适当复制元素使这两个 Tensor 形状相同后再按元素运算。例如：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = t.arange(<span class="number">1</span>, <span class="number">3</span>).view(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">print(x)</span><br><span class="line">y = t.arange(<span class="number">1</span>, <span class="number">4</span>).view(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">print(y)</span><br><span class="line">print(x + y)</span><br></pre></td></tr></table></figure><br>输出结果<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>]])</span><br><span class="line">tensor([[<span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>]])</span><br><span class="line">tensor([[<span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>]])</span><br></pre></td></tr></table></figure><br>由于 x 和 y 分别是 1 行 2 列和 3 行 1 列的矩阵，如果要计算 x + y，那么 x 中第一行的 2 个元素被广播（复制）到了第二行和第三行，而 y 中第一列的 3 个元素被广播（复制）到了第二列。如此，就可以对 2 个 3 行 2 列的矩阵按元素相加。<p></p><h3 id="运算的内存开销"><a class="anchor" href="#运算的内存开销">#</a> 运算的内存开销</h3><p>前面说了，索引操作是不会开辟新内存的，而像 y = x + y 这样的运算是会新开内存的，然后将 y 指向新内存。可以使用 Python 自带的 id 函数：如果两个实例的 ID 一致，那么它们所对应的内存地址相同；反之则不同。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = t.tensor([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">y = t.tensor([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">id_before = <span class="built_in">id</span>(y)</span><br><span class="line">y = y + x</span><br><span class="line">print(<span class="built_in">id</span>(y) == id_before) <span class="comment"># False </span></span><br></pre></td></tr></table></figure><br>如果想指定结果到原来的 y 的内存，我们可以使用前面介绍的索引来进行替换操作。在下面的例子中，我们把 x + y 的结果通过 [:] 写进 y 对应的内存中。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = t.tensor([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">y = t.tensor([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">id_before = <span class="built_in">id</span>(y)</span><br><span class="line">y[:] = y + x</span><br><span class="line">print(<span class="built_in">id</span>(y) == id_before) <span class="comment"># True</span></span><br></pre></td></tr></table></figure><br>还可以使用运算符全名函数中的 out 参数或者自加运算符 +=(也即 add _ ()) 达到上述效果，例如 torch.add (x, y, out=y) 和 y += x (y.add_(x))。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = t.tensor([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">y = t.tensor([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">id_before = <span class="built_in">id</span>(y)</span><br><span class="line">t.add(x, y, out=y) <span class="comment"># y += x, y.add_(x)</span></span><br><span class="line">print(<span class="built_in">id</span>(y) == id_before) <span class="comment"># True</span></span><br></pre></td></tr></table></figure><br>注意：]{.red} 虽然 view 返回的 Tensor 与源 Tensor 是共享 data 的，但是依然是一个新的 Tensor（因为 Tensor 除了包含 data 外还有一些其他属性），二者 id（内存地址）并不一致。<p></p><h3 id="tensor和numpy相互转换"><a class="anchor" href="#tensor和numpy相互转换">#</a> Tensor 和 NumPy 相互转换</h3><p>我们很容易用 numpy () 和 from_numpy () 将 Tensor 和 NumPy 中的数组相互转换。但是需要注意的一点是： 这两个函数所产生的的 Tensor 和 NumPy 中的数组共享相同的内存（所以他们之间的转换很快），改变其中一个时另一个也会改变！！！</p><ul><li>Tensor 转 NumPy</li></ul><p>使用 numpy () 将 Tensor 转换成 NumPy 数组:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a=t.ones(<span class="number">5</span>)</span><br><span class="line">b=a.numpy()</span><br><span class="line">print(a,b)</span><br><span class="line"></span><br><span class="line">a+=<span class="number">1</span></span><br><span class="line">print(a,b)</span><br><span class="line"></span><br><span class="line">b+=<span class="number">1</span></span><br><span class="line">print(a,b)</span><br></pre></td></tr></table></figure><br>输出结果<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]) [<span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line">tensor([<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>]) [<span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span>]</span><br><span class="line">tensor([<span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>]) [<span class="number">3.</span> <span class="number">3.</span> <span class="number">3.</span> <span class="number">3.</span> <span class="number">3.</span>]</span><br></pre></td></tr></table></figure><p></p><ul><li>NumPy 数组转 Tensor</li></ul><p>使用 from_numpy () 将 NumPy 数组转换成 Tensor。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.ones(<span class="number">5</span>)</span><br><span class="line">b = t.from_numpy(a)</span><br><span class="line">print(a, b)</span><br><span class="line"></span><br><span class="line">a += <span class="number">1</span></span><br><span class="line">print(a, b)</span><br><span class="line">b += <span class="number">1</span></span><br><span class="line">print(a, b)</span><br></pre></td></tr></table></figure><br>输出结果<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span>] tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>], dtype=torch.float64)</span><br><span class="line">[<span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span>] tensor([<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>], dtype=torch.float64)</span><br><span class="line">[<span class="number">3.</span> <span class="number">3.</span> <span class="number">3.</span> <span class="number">3.</span> <span class="number">3.</span>] tensor([<span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>], dtype=torch.float64)</span><br></pre></td></tr></table></figure><p></p><p>所有在 CPU 上的 Tensor（除了 CharTensor）都支持与 NumPy 数组相互转换。</p><p>此外上面提到还有一个常用的方法就是直接用 torch.tensor () 将 NumPy 数组转换成 Tensor，需要注意的是该方法总是会进行数据拷贝，返回的 Tensor 和原来的数据不再共享内存。</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">c = t.tensor(a)</span><br><span class="line">a += <span class="number">1</span></span><br><span class="line">print(a, c)</span><br></pre></td></tr></table></figure><br>输出结果<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">4.</span> <span class="number">4.</span> <span class="number">4.</span> <span class="number">4.</span> <span class="number">4.</span>] tensor([<span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>], dtype=torch.float64)</span><br></pre></td></tr></table></figure><p></p><h3 id="tensor-on-gpu"><a class="anchor" href="#tensor-on-gpu">#</a> Tensor on GPU</h3><p>用方法 to () 可以将 Tensor 在 CPU 和 GPU（需要硬件支持）之间相互移动。</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以下代码只有在PyTorch GPU版本上才会执行</span></span><br><span class="line"><span class="keyword">if</span> t.cuda.is_available():   </span><br><span class="line">    device=t.device(<span class="string">&quot;cuda&quot;</span>)  <span class="comment"># GPU</span></span><br><span class="line">    y=t.ones_like(x,device=device)   <span class="comment"># 直接创建一个在GPU上的Tensor</span></span><br><span class="line">    x=x.to(device)  <span class="comment"># 等价于 .to(&quot;cuda&quot;)</span></span><br><span class="line">    z=x + y</span><br><span class="line">    print(z)</span><br><span class="line">    print(z.to(<span class="string">&quot;cpu&quot;</span>,t.double))   <span class="comment"># to()还可以同时更改数据类型</span></span><br></pre></td></tr></table></figure><p></p><h2 id="自动求梯度微分"><a class="anchor" href="#自动求梯度微分">#</a> 自动求梯度 (微分)</h2><p>深度学习的算法本质上是通过反向传播求导数，而 PyTorch 的 ** <code>autograd</code> ** 模块则实现了此功能。在 Tensor 上的所有操作，autograd 都能为它们自动提供微分，避免了手动计算导数的复杂过程。</p><p>上一节介绍的 Tensor 是这个包的核心类，如果将其属性.requires_grad 设置为 ** <code>True</code> **，它将开始追踪 (track) 在其上的所有操作（这样就可以利用链式法则进行梯度传播了）。完成计算后，可以调用.backward () 来完成所有梯度计算。此 Tensor 的梯度将累积到.grad 属性中。</p><p>注意：]{.red} 在 y.backward () 时，如果 y 是标量，则不需要为 backward () 传入任何参数；否则，需要传入一个与 y 同形的 Tensor。</p><p>如果不想要被继续追踪，可以调用.detach () 将其从追踪记录中分离出来，这样就可以防止将来的计算被追踪，这样梯度就传不过去了。此外，还可以用 with torch.no_grad () 将不想被追踪的操作代码块包裹起来，这种方法在评估模型的时候很常用，因为在评估模型时，我们并不需要计算可训练参数（requires_grad=True）的梯度。</p><p>Function 是另外一个很重要的类。Tensor 和 Function 互相结合就可以构建一个记录有整个计算过程的有向无环图（DAG）。每个 Tensor 都有一个.grad_fn 属性，该属性即创建该 Tensor 的 Function, 就是说该 Tensor 是不是通过某些运算得到的，若是，则 grad_fn 返回一个与这些运算相关的对象，否则是 None。</p><p>下面通过一些例子来理解这些概念。</p><h3 id="tensor"><a class="anchor" href="#tensor">#</a> Tensor</h3><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x=t.ones(<span class="number">2</span>,<span class="number">2</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">print(x)</span><br><span class="line">print(x.grad_fn)</span><br></pre></td></tr></table></figure><p></p><p>输出结果</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="literal">None</span></span><br></pre></td></tr></table></figure><br>再做一下运算操作<p></p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y=x + <span class="number">2</span></span><br><span class="line">print(y)</span><br><span class="line">print(y.grad_fn)</span><br></pre></td></tr></table></figure><p></p><p>输出结果</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">3.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">3.</span>]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">&lt;AddBackward0 <span class="built_in">object</span> at <span class="number">0x0000023F5436D668</span>&gt;</span><br></pre></td></tr></table></figure><p></p><p>注意：]{.red} x 是直接创建的，所以它没有 grad_fn, 而 y 是通过一个加法操作创建的，所以它有一个为 &lt; AddBackward &gt; 的 grad_fn。像 x 这种直接创建的称为叶子节点，叶子节点对应的 grad_fn 是 None。</p><p>再来点复杂度运算操作</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">z=y*y*<span class="number">3</span></span><br><span class="line">out=z.mean()</span><br><span class="line">print(z,out)</span><br></pre></td></tr></table></figure><p></p><p>输出结果</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">27.</span>, <span class="number">27.</span>],</span><br><span class="line">        [<span class="number">27.</span>, <span class="number">27.</span>]], grad_fn=&lt;MulBackward0&gt;) tensor(<span class="number">27.</span>, grad_fn=&lt;MeanBackward0&gt;)</span><br></pre></td></tr></table></figure><p></p><p>通过.requires_grad_() 来用 in-place 的方式改变 requires_grad 属性。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a=t.randn(<span class="number">2</span>,<span class="number">2</span>)  <span class="comment"># 缺失情况下默认 requires_grad = False</span></span><br><span class="line">a=((a*<span class="number">3</span>)/(a-<span class="number">1</span>))</span><br><span class="line">print(a.requires_grad)  <span class="comment"># False</span></span><br><span class="line">a.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">print(a.requires_grad)   <span class="comment"># True</span></span><br><span class="line">b=(a*a).<span class="built_in">sum</span>()</span><br><span class="line">print(b.grad_fn)</span><br></pre></td></tr></table></figure><p></p><p>输出结果</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line">&lt;SumBackward0 <span class="built_in">object</span> at <span class="number">0x0000023F5436DC88</span>&gt;</span><br></pre></td></tr></table></figure><p></p><h3 id="梯度"><a class="anchor" href="#梯度">#</a> 梯度</h3><p>因为 out 是一个标量，所以调用 backward () 时不需要指定求导变量<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out.backward()</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(x)</span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure><br>输出结果<p></p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">tensor([[<span class="number">4.5000</span>, <span class="number">4.5000</span>],</span><br><span class="line">        [<span class="number">4.5000</span>, <span class="number">4.5000</span>]])</span><br></pre></td></tr></table></figure><br><img data-src="/images/textimages/text59/4.png" alt="4.png | center | 300x0"><p></p><p>注意：]{.red} grad 在反向传播过程中是累加的 (accumulated)，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以一般在反向传播之前需把梯度清零。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 再来反向传播一次，注意grad是累加的</span></span><br><span class="line">out2=x.<span class="built_in">sum</span>()</span><br><span class="line">out2.backward()</span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure><br>输出结果<p></p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">5.5000</span>, <span class="number">5.5000</span>],</span><br><span class="line">        [<span class="number">5.5000</span>, <span class="number">5.5000</span>]])</span><br></pre></td></tr></table></figure><p></p><p><strong>总结:</strong> 为什么在 y.backward () 时，如果 y 是标量，则不需要为 backward () 传入任何参数；否则，需要传入一个与 y 同形的 Tensor? <strong>简单来说就是为了避免向量（甚至更高维张量）对张量求导，而转换成标量对张量求导。举个例子，假设形状为 m x n 的矩阵 X 经过运算得到了 p x q 的矩阵 Y，Y 又经过运算得到了 s x t 的矩阵 Z。那么按照前面讲的规则，dZ/dY 应该是一个 s x t x p x q 四维张量，dY/dX 是一个 p x q x m x n 的四维张量。问题来了，怎样反向传播？怎样将两个四维张量相乘？？？这要怎么乘？？？就算能解决两个四维张量怎么乘的问题，四维和三维的张量又怎么乘？导数的导数又怎么求，这一连串的问题，感觉要疯掉…… 为了避免这个问题，我们不允许张量对张量求导，<strong>只允许标量对张量求导，求导结果是和自变量同形的张量</strong>。所以必要时我们要把</strong>张量通过将所有张量的元素加权求和的方式转换为标量 **，举个例子，假设 y 由自变量 x 计算而来，w 是和 y 同形的张量，则 y.backward (w) 的含义是：先计算 l = torch.sum (y * w)，则 l 是个标量，然后求 l 对自变量 x 的导数。</p><p>来看一些实际例子。</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x=t.tensor([<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">3.0</span>,<span class="number">4.0</span>],requires_grad=<span class="literal">True</span>)</span><br><span class="line">y=<span class="number">2</span> * x</span><br><span class="line">z=y.view(<span class="number">2</span>,<span class="number">2</span>)  <span class="comment"># 改变形状</span></span><br><span class="line">print(z)</span><br></pre></td></tr></table></figure><br>输出结果<p></p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">2.</span>, <span class="number">4.</span>],</span><br><span class="line">        [<span class="number">6.</span>, <span class="number">8.</span>]], grad_fn=&lt;ViewBackward&gt;)</span><br></pre></td></tr></table></figure><br>现在 y 不是一个标量，所以在调用 backward 时需要传入一个和 y 同形的权重向量进行加权求和得到一个标量。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">v = t.tensor([[<span class="number">1.0</span>, <span class="number">0.1</span>], [<span class="number">0.01</span>, <span class="number">0.001</span>]], dtype=t.<span class="built_in">float</span>)</span><br><span class="line">z.backward(v)</span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure><br>输出结果<p></p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">2.0000</span>, <span class="number">0.2000</span>, <span class="number">0.0200</span>, <span class="number">0.0020</span>])</span><br></pre></td></tr></table></figure><br>注意：]{.red} x.grad 是和 x 同形的张量。<p></p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x=t.tensor(<span class="number">1.0</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">y1=x ** <span class="number">3</span></span><br><span class="line"><span class="keyword">with</span> t.no_grad():</span><br><span class="line">    y2=x ** <span class="number">3</span></span><br><span class="line">y3=y1 + y2</span><br><span class="line">print(x.requires_grad)</span><br><span class="line">print(y1,y1.requires_grad)  <span class="comment"># True</span></span><br><span class="line">print(y2,y2.requires_grad)   <span class="comment"># False</span></span><br><span class="line">print(y3,y3.requires_grad)  <span class="comment"># True</span></span><br></pre></td></tr></table></figure><br>输出结果<p></p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">True</span></span><br><span class="line">tensor(<span class="number">1.</span>, grad_fn=&lt;PowBackward0&gt;) <span class="literal">True</span></span><br><span class="line">tensor(<span class="number">1.</span>) <span class="literal">False</span></span><br><span class="line">tensor(<span class="number">2.</span>, grad_fn=&lt;AddBackward0&gt;) <span class="literal">True</span></span><br></pre></td></tr></table></figure><p></p><p>可以看到，上面的 y2 是没有 grad _ fn 而且 y2.requires_ grad=False 的，而 y3 是有 grad_fn 的。如果我们将 y3 对 x 求梯度的话会是多少呢？<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y3.backward()</span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure><br>输出结果<p></p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">2.</span>)</span><br></pre></td></tr></table></figure><br>为什么是 2 呢？<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mn>3</mn></msub><mo>=</mo><msub><mi>y</mi><mn>1</mn></msub><mo>+</mo><msub><mi>y</mi><mn>2</mn></msub><mo>=</mo><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><msup><mi>x</mi><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">y_3=y_1+y_2=x^2+x^3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.7777700000000001em;vertical-align:-.19444em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.897438em;vertical-align:-.08333em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141079999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.8141079999999999em;vertical-align:0"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141079999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span>，当 x=1 时，$ \frac {dy_3}{dx} $ 不应该是 5 吗？ 这是因为 $ y_2 $ 的定义是被 torch.no_grad (): 包裹的，所以与 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">y_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> 有关的梯度是不会回传的，只有与 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">y_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> 有关的梯度才会回传，即 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">x^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8141079999999999em;vertical-align:0"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141079999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> 对 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">x</span></span></span></span> 的梯度。这里 y2.requires_grad=False，所以不能调用 y2.backward ()，会报错。<p></p><p>如果我们想要修改 tensor 的数值，但是又不希望被 autograd 记录（即不会影响反向传播），那么我么可以对 tensor.data 进行操作。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x=t.ones(<span class="number">1</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">print(x.data)  <span class="comment"># 还是一个tensor</span></span><br><span class="line">print(x.data.requires_grad)   <span class="comment"># 但是已经是独立于计算图之外</span></span><br><span class="line"></span><br><span class="line">y=<span class="number">2</span> * x</span><br><span class="line">x.data *=<span class="number">100</span>  <span class="comment"># 只改变了值，不会记录在计算图，所以不会影响梯度传播</span></span><br><span class="line"></span><br><span class="line">y.backward()</span><br><span class="line">print(x)  <span class="comment"># 更改data的值也会影响tensor的值</span></span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure><br>输出结果<p></p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">1.</span>])</span><br><span class="line"><span class="literal">False</span></span><br><span class="line">tensor([<span class="number">100.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">tensor([<span class="number">2.</span>])</span><br></pre></td></tr></table></figure><p></p><div class="tags"><a href="/tags/PyTorch-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="ic i-tag"></i> PyTorch 深度学习</a></div></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2020-12-28 19:52:00" itemprop="dateModified" datetime="2020-12-28T19:52:00+08:00">2020-12-28</time> </span><span id="2019/10/11/computer-vision/DeepLearning/text59/" class="item leancloud_visitors" data-flag-title="PyTorch 快速入门" title="阅读次数"><span class="icon"><i class="ic i-eye"></i> </span><span class="text">阅读次数</span> <span class="leancloud-visitors-count"></span> <span class="text">次</span></span></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>Mr.Wang <i class="ic i-at"><em>@</em></i>一只行走的红烧肉</li><li class="link"><strong>本文链接：</strong> <a href="http://example.com/2019/10/11/computer-vision/DeepLearning/text59/" title="PyTorch 快速入门">http://example.com/2019/10/11/computer-vision/DeepLearning/text59/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/2019/10/10/computer-vision/DeepLearning/text58/" itemprop="url" rel="prev" data-background-image="http:&#x2F;&#x2F;pic.yupoo.com&#x2F;1241716616&#x2F;caf26d93&#x2F;80e1f66e.jpg" title="PyTorch 开发环境的搭建"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> 深度学习</span><h3>PyTorch 开发环境的搭建</h3></a></div><div class="item right"><a href="/2019/10/12/computer-vision/DeepLearning/text60/" itemprop="url" rel="next" data-background-image="http:&#x2F;&#x2F;pic.yupoo.com&#x2F;1241716616&#x2F;b3bc9882&#x2F;2cc06e1b.jpg" title="深度学习基础-线性回归"><span class="type">下一篇</span> <span class="category"><i class="ic i-flag"></i> 深度学习</span><h3>深度学习基础-线性回归</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#pytorch-%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8"><span class="toc-number">1.</span> <span class="toc-text">PyTorch 快速入门</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C"><span class="toc-number">1.1.</span> <span class="toc-text">数据操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-tensor"><span class="toc-number">1.1.1.</span> <span class="toc-text">创建 Tensor</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%9C%AF%E6%93%8D%E4%BD%9C"><span class="toc-number">1.1.2.</span> <span class="toc-text">算术操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B9%BF%E6%92%AD%E6%9C%BA%E5%88%B6"><span class="toc-number">1.1.3.</span> <span class="toc-text">广播机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%90%E7%AE%97%E7%9A%84%E5%86%85%E5%AD%98%E5%BC%80%E9%94%80"><span class="toc-number">1.1.4.</span> <span class="toc-text">运算的内存开销</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tensor%E5%92%8Cnumpy%E7%9B%B8%E4%BA%92%E8%BD%AC%E6%8D%A2"><span class="toc-number">1.1.5.</span> <span class="toc-text">Tensor 和 NumPy 相互转换</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tensor-on-gpu"><span class="toc-number">1.1.6.</span> <span class="toc-text">Tensor on GPU</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E6%B1%82%E6%A2%AF%E5%BA%A6%E5%BE%AE%E5%88%86"><span class="toc-number">1.2.</span> <span class="toc-text">自动求梯度 (微分)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#tensor"><span class="toc-number">1.2.1.</span> <span class="toc-text">Tensor</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6"><span class="toc-number">1.2.2.</span> <span class="toc-text">梯度</span></a></li></ol></li></ol></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li><a href="/2019/10/10/computer-vision/DeepLearning/text58/" rel="bookmark" title="PyTorch 开发环境的搭建">PyTorch 开发环境的搭建</a></li><li class="active"><a href="/2019/10/11/computer-vision/DeepLearning/text59/" rel="bookmark" title="PyTorch 快速入门">PyTorch 快速入门</a></li><li><a href="/2019/10/12/computer-vision/DeepLearning/text60/" rel="bookmark" title="深度学习基础-线性回归">深度学习基础-线性回归</a></li><li><a href="/2019/10/13/computer-vision/DeepLearning/text61/" rel="bookmark" title="深度学习基础-从零实现线性回归">深度学习基础-从零实现线性回归</a></li><li><a href="/2019/10/14/computer-vision/DeepLearning/text62/" rel="bookmark" title="深度学习基础-从零实现线性回归II">深度学习基础-从零实现线性回归II</a></li><li><a href="/2019/10/15/computer-vision/DeepLearning/text63/" rel="bookmark" title="深度学习基础-softmax回归">深度学习基础-softmax回归</a></li><li><a href="/2019/10/16/computer-vision/DeepLearning/text64/" rel="bookmark" title="深度学习基础-图像数据集">深度学习基础-图像数据集</a></li><li><a href="/2019/10/17/computer-vision/DeepLearning/text65/" rel="bookmark" title="深度学习基础-从零实现 softmax 回归">深度学习基础-从零实现 softmax 回归</a></li><li><a href="/2019/10/18/computer-vision/DeepLearning/text66/" rel="bookmark" title="深度学习基础-softmax回归的简洁实现">深度学习基础-softmax回归的简洁实现</a></li><li><a href="/2019/10/19/computer-vision/DeepLearning/text67/" rel="bookmark" title="深度学习基础-多层感知机">深度学习基础-多层感知机</a></li><li><a href="/2019/10/20/computer-vision/DeepLearning/text68/" rel="bookmark" title="深度学习基础-从零实现多层感知机">深度学习基础-从零实现多层感知机</a></li><li><a href="/2019/10/22/computer-vision/DeepLearning/text69/" rel="bookmark" title="深度学习基础-模型选择、欠拟合和过拟合">深度学习基础-模型选择、欠拟合和过拟合</a></li><li><a href="/2019/10/23/computer-vision/DeepLearning/text70/" rel="bookmark" title="深度学习基础-权重衰减">深度学习基础-权重衰减</a></li><li><a href="/2019/10/24/computer-vision/DeepLearning/text71/" rel="bookmark" title="深度学习基础-丢弃法">深度学习基础-丢弃法</a></li><li><a href="/2019/10/25/computer-vision/DeepLearning/text72/" rel="bookmark" title="深度学习基础-正向传播、反向传播和计算图">深度学习基础-正向传播、反向传播和计算图</a></li><li><a href="/2019/10/26/computer-vision/DeepLearning/text73/" rel="bookmark" title="深度学习基础-数值稳定性和模型初始化">深度学习基础-数值稳定性和模型初始化</a></li><li><a href="/2019/10/28/computer-vision/DeepLearning/text74/" rel="bookmark" title="深度学习基础-数值稳定性和模型初始化">深度学习基础-数值稳定性和模型初始化</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="Mr.Wang" data-src="/images/author.jpg"><p class="name" itemprop="name">Mr.Wang</p><div class="description" itemprop="description">一只行走的红烧肉</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">78</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">9</span> <span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">12</span> <span class="name">标签</span></a></div></nav><div class="social"><span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tLzEyNDE3MTY2MTY=" title="https:&#x2F;&#x2F;github.com&#x2F;1241716616"><i class="ic i-github"></i></span> <span class="exturl item zhihu" data-url="aHR0cHM6Ly93d3cuemhpaHUuY29tL3Blb3BsZS95b3VybmFtZQ==" title="https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;yourname"><i class="ic i-zhihu"></i></span> <span class="exturl item music" data-url="aHR0cHM6Ly9tdXNpYy4xNjMuY29tLyMvdXNlci9ob21lP2lkPTIxNDE1NzI1MjA=" title="https:&#x2F;&#x2F;music.163.com&#x2F;#&#x2F;user&#x2F;home?id&#x3D;2141572520"><i class="ic i-cloud-music"></i></span> <span class="exturl item weibo" data-url="aHR0cHM6Ly93ZWliby5jb20veW91cm5hbWU=" title="https:&#x2F;&#x2F;weibo.com&#x2F;yourname"><i class="ic i-weibo"></i></span> <span class="exturl item about" data-url="aHR0cHM6Ly9hYm91dC5tZS95b3VybmFtZQ==" title="https:&#x2F;&#x2F;about.me&#x2F;yourname"><i class="ic i-address-card"></i></span> <span class="exturl item email" data-url="bWFpbHRvOjEyNDE3MTY2MTZAcXEuY29t" title="mailto:1241716616@qq.com"><i class="ic i-envelope"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item"><a href="/about/" rel="section"><i class="ic i-user"></i>关于</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>friends</a></li><li class="item"><a href="/links/" rel="section"><i class="ic i-magic"></i>links</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/2019/10/10/computer-vision/DeepLearning/text58/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/2019/10/12/computer-vision/DeepLearning/text60/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 计算机科学">计算机科学</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/cpp/" title="分类于 C++基础知识">C++基础知识</a></div><span><a href="/2018/09/28/computer-science/cpp/text9/" title="STL-标准模板库">STL-标准模板库</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-vision/" title="分类于 计算机视觉">计算机视觉</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-vision/Opencv/" title="分类于 OpenCV图像处理算法">OpenCV图像处理算法</a></div><span><a href="/2019/05/12/computer-vision/text14/" title="定义感兴趣区">定义感兴趣区</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 计算机科学">计算机科学</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/cpp/" title="分类于 C++基础知识">C++基础知识</a></div><span><a href="/2018/10/12/computer-science/cpp/text13/" title="演讲比赛流程管理系统">演讲比赛流程管理系统</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-vision/" title="分类于 计算机视觉">计算机视觉</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-vision/DeepLearning/" title="分类于 深度学习">深度学习</a></div><span><a href="/2019/10/24/computer-vision/DeepLearning/text71/" title="深度学习基础-丢弃法">深度学习基础-丢弃法</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-vision/" title="分类于 计算机视觉">计算机视觉</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-vision/Opencv/" title="分类于 OpenCV图像处理算法">OpenCV图像处理算法</a></div><span><a href="/2019/07/11/computer-vision/text40/" title="视频分析与对象跟踪-OpenCV 扩展模块的单目标、多目标跟踪">视频分析与对象跟踪-OpenCV 扩展模块的单目标、多目标跟踪</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-vision/" title="分类于 计算机视觉">计算机视觉</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-vision/DeepLearning/" title="分类于 深度学习">深度学习</a></div><span><a href="/2019/10/19/computer-vision/DeepLearning/text67/" title="深度学习基础-多层感知机">深度学习基础-多层感知机</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 计算机科学">计算机科学</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/cpp/" title="分类于 C++基础知识">C++基础知识</a></div><span><a href="/2018/09/20/computer-science/cpp/text5/" title="C++文件操作">C++文件操作</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 计算机科学">计算机科学</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/cpp/" title="分类于 C++基础知识">C++基础知识</a></div><span><a href="/2018/09/17/computer-science/cpp/text3/" title="通讯录管理系统">通讯录管理系统</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 计算机科学">计算机科学</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/Webspider/" title="分类于 网页爬虫">网页爬虫</a></div><span><a href="/2019/09/08/computer-science/Webspider/text56/" title="使用 Selenium 模拟浏览器抓取淘宝西红柿信息">使用 Selenium 模拟浏览器抓取淘宝西红柿信息</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 计算机科学">计算机科学</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/Webspider/" title="分类于 网页爬虫">网页爬虫</a></div><span><a href="/2019/08/28/computer-science/Webspider/text50/" title="数据解析">数据解析</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2018 – <span itemprop="copyrightYear">2020</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">Mr.Wang @ 业余天王</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">467k 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">7:05</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"2019/10/11/computer-vision/DeepLearning/text59/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script></body></html>